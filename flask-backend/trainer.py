from flask import Flask, request, jsonify
import requests
from pathlib import Path
import os
import cv2
import shutil
from ultralytics import YOLO
import yaml
import torch

app = Flask(__name__)

TRAIN_DIR = "./train_data"

#ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
# ./train_data/target{targetId}/{1ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ}.jpg êµ¬ì¡°ë¡œ ì €ì¥í•˜ê³  "./train_data/target{targetId}" ê²½ë¡œ ë°˜í™˜
def download_images(target_id):
    base_url = f"https://dongmul-default-rtdb.firebaseio.com/animalList/target{target_id}/fileURL"
    # fileURL ë²„í‚· ì•ˆì— num ê°€ì ¸ì˜¤ê¸°
    num_res = requests.get(f"{base_url}/num.json")
    if num_res.status_code != 200:

        return False, "Failed to get num"
    num_files = num_res.json()
    if not num_files or num_files == 0:
        return False, "No images to download"

    label_dir = Path(TRAIN_DIR) / f"target{target_id}"
    label_dir.mkdir(parents=True, exist_ok=True) #./train_dataê°€ ì—†ì–´ë„ ìë™ìœ¼ë¡œ ìƒì„±, exist_ok=True: ì´ë¯¸ í´ë”ê°€ ì¡´ì¬í•˜ë©´ ì—ëŸ¬ ì—†ì´ ê·¸ëƒ¥ ë„˜ì–´ê°

    downloaded_count = 0

    for i in range(1, num_files + 1):
        url_res = requests.get(f"{base_url}/url{i}.json")
        if url_res.status_code != 200:
            print(f"Failed to get url{i}")
            continue
        url = url_res.json()
        try:
            img_res = requests.get(url) # â‘  ì´ë¯¸ì§€ URLë¡œ ìš”ì²­ ë³´ë‚´ê¸°
            img_res.raise_for_status() # â‘¡ ì‘ë‹µ ìƒíƒœ í™•ì¸ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸)

            #  í™•ì¥ì ìë™ ì¶”ì¶œ
            extension = os.path.splitext(url.split("?")[0])[1]  # URLì—ì„œ í™•ì¥ì ì¶”ì¶œ (ì¿¼ë¦¬ìŠ¤íŠ¸ë§ ì œê±°)
            if not extension:  
                extension = ".jpg"  # í™•ì¥ì ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ jpg
            
            #  í™•ì¥ì ë°˜ì˜í•´ì„œ ì €ì¥
            with open(label_dir / f"{i}{extension}", "wb") as f:
                f.write(img_res.content)
            downloaded_count += 1

        except Exception as e:
            print(f"Error downloading image {url}: {e}")
    if downloaded_count == 0:
        return False, "All downloads failed"

    return True, str(label_dir)

# â‘¥ ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ << ì½”ë“œ ë„£ì–´ì£¼ì„¸ìš”!
#ì–´ë–¤ ë°ì´í„° í•„ìš”í•˜ì‹ ì§€ ëª°ë¼ì„œ ì¼ë‹¨ ì´ë¯¸ì§€ ê²½ë¡œë§Œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤
################################################################\

def fix_data_yaml(dataset_folder):
    """
    data.yaml íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ìˆ˜ì •
    
    Args:
        dataset_folder (str): ë°ì´í„°ì…‹ í´ë” ê²½ë¡œ
    """
    dataset_path = Path(dataset_folder)
    yaml_path = dataset_path / 'data.yaml'
    
    if not yaml_path.exists():
        raise FileNotFoundError(f"data.yaml íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {yaml_path}")
    
    # í˜„ì¬ yaml íŒŒì¼ ì½ê¸°
    with open(yaml_path, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½
    current_dir = os.getcwd()
    
    # ê° splitì˜ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½
    for split in ['train', 'val', 'test']:
        if split in data:
            relative_path = data[split]
            # í˜„ì¬ ê²½ë¡œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
            absolute_path = os.path.abspath(os.path.join(current_dir, relative_path))
            data[split] = absolute_path
            print(f"{split} ê²½ë¡œ: {absolute_path}")
    
    # ìˆ˜ì •ëœ yaml íŒŒì¼ ì €ì¥
    with open(yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(data, f, default_flow_style=False)
    
    print(f"data.yaml íŒŒì¼ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {yaml_path}")
    return str(yaml_path)

def check_dataset_structure(dataset_folder):
    """
    ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸ ë° ëˆ„ë½ëœ í´ë” ìƒì„±
    
    Args:
        dataset_folder (str): ë°ì´í„°ì…‹ í´ë” ê²½ë¡œ
    """
    dataset_path = Path(dataset_folder)
    
    print("ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸ ì¤‘...")
    
    # í•„ìš”í•œ í´ë” êµ¬ì¡°
    required_dirs = [
        'train/images', 'train/labels',
        'val/images', 'val/labels',
        'test/images', 'test/labels'
    ]
    
    for dir_path in required_dirs:
        full_path = dataset_path / dir_path
        if not full_path.exists():
            print(f"ëˆ„ë½ëœ í´ë” ìƒì„±: {full_path}")
            full_path.mkdir(parents=True, exist_ok=True)
        else:
            # í´ë” ë‚´ íŒŒì¼ ê°œìˆ˜ í™•ì¸
            if 'images' in dir_path:
                image_count = len(list(full_path.glob('*.jpg'))) + len(list(full_path.glob('*.png')))
                print(f"{dir_path}: {image_count}ê°œ ì´ë¯¸ì§€")
            elif 'labels' in dir_path:
                label_count = len(list(full_path.glob('*.txt')))
                print(f"{dir_path}: {label_count}ê°œ ë¼ë²¨")
    
    # data.yaml íŒŒì¼ í™•ì¸
    yaml_path = dataset_path / 'data.yaml'
    if yaml_path.exists():
        with open(yaml_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        print(f"í´ë˜ìŠ¤ ìˆ˜: {data.get('nc', 'N/A')}")
        print(f"í´ë˜ìŠ¤ ì´ë¦„: {data.get('names', 'N/A')}")
    else:
        print("data.yaml íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    return True

def create_minimal_dataset_if_missing(dataset_folder):
    """
    ë°ì´í„°ì…‹ì´ ì—†ëŠ” ê²½ìš° ìµœì†Œí•œì˜ êµ¬ì¡° ìƒì„±
    """
    dataset_path = Path(dataset_folder)
    
    if not dataset_path.exists():
        print(f"ë°ì´í„°ì…‹ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒì„±í•©ë‹ˆë‹¤: {dataset_path}")
        dataset_path.mkdir(parents=True, exist_ok=True)
        
        # ê¸°ë³¸ í´ë” êµ¬ì¡° ìƒì„±
        for split in ['train', 'val', 'test']:
            (dataset_path / split / 'images').mkdir(parents=True, exist_ok=True)
            (dataset_path / split / 'labels').mkdir(parents=True, exist_ok=True)
        
        # ê¸°ë³¸ data.yaml ìƒì„±
        default_yaml = {
            'train': str(dataset_path / 'train' / 'images'),
            'val': str(dataset_path / 'val' / 'images'),
            'test': str(dataset_path / 'test' / 'images'),
            'nc': 2,
            'names': ['target1', 'target2']
        }
        
        with open(dataset_path / 'data.yaml', 'w', encoding='utf-8') as f:
            yaml.dump(default_yaml, f, default_flow_style=False)
        
        print("ê¸°ë³¸ ë°ì´í„°ì…‹ êµ¬ì¡°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ì‹¤ì œ ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ì„ í•´ë‹¹ í´ë”ì— ë„£ì–´ì£¼ì„¸ìš”.")

def fixed_train():
    """ìˆ˜ì •ëœ í•™ìŠµ í•¨ìˆ˜"""
    
    # GPU í™•ì¸
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ
    dataset_folder = "generated_animal_dataset"
    
    try:
        # 1. ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
        if not os.path.exists(dataset_folder):
            print(f"ë°ì´í„°ì…‹ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_folder}")
            print("ë¨¼ì € ë°ì´í„°ì…‹ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
            create_minimal_dataset_if_missing(dataset_folder)
            return
        
        # 2. ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
        check_dataset_structure(dataset_folder)
        
        # 3. data.yaml ê²½ë¡œ ìˆ˜ì •
        yaml_path = fix_data_yaml(dataset_folder)
        
        # 4. ëª¨ë¸ ë¡œë“œ
        model = YOLO('yolov8s.pt')
        
        # 5. í•™ìŠµ ì‹œì‘
        print("\n" + "="*50)
        print("í•™ìŠµ ì‹œì‘...")
        print("="*50)
        
        results = model.train(
            data=yaml_path,
            epochs=100,
            imgsz=640,
            batch=16,
            device=device,
            project='animal_detection',
            name='train',
            exist_ok=True,
            patience=50,
            save_period=10,
            plots=True,
            val=True,
            verbose=True
        )
        
        print("\n" + "="*50)
        print("í•™ìŠµ ì™„ë£Œ!")
        print("="*50)
        print("ìµœê³  ëª¨ë¸: animal_detection/train/weights/best.pt")
        print("ìµœì¢… ëª¨ë¸: animal_detection/train/weights/last.pt")
        print("í•™ìŠµ ê²°ê³¼: animal_detection/train/")
        
        # 6. ê²€ì¦
        print("\nê²€ì¦ ì‹œì‘...")
        best_model = YOLO('animal_detection/train/weights/best.pt')
        val_results = best_model.val(data=yaml_path)
        
        print("ê²€ì¦ ì™„ë£Œ!")
        return results
        
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print("\në¬¸ì œ í•´ê²° ë°©ë²•:")
        print("1. ë°ì´í„°ì…‹ í´ë”ê°€ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸")
        print("2. ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ì´ ì˜¬ë°”ë¥¸ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸")
        print("3. data.yaml íŒŒì¼ ë‚´ìš© í™•ì¸")
        raise

def debug_dataset_paths():
    """ë°ì´í„°ì…‹ ê²½ë¡œ ë””ë²„ê¹…"""
    dataset_folder = "generated_animal_dataset"
    dataset_path = Path(dataset_folder)
    
    print("=" * 50)
    print("ë°ì´í„°ì…‹ ê²½ë¡œ ë””ë²„ê¹…")
    print("=" * 50)
    
    print(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print(f"ë°ì´í„°ì…‹ í´ë” ì¡´ì¬: {dataset_path.exists()}")
    
    if dataset_path.exists():
        print(f"ë°ì´í„°ì…‹ ì ˆëŒ€ ê²½ë¡œ: {dataset_path.absolute()}")
        
        # ê° í´ë” í™•ì¸
        for split in ['train', 'val', 'test']:
            images_path = dataset_path / split / 'images'
            labels_path = dataset_path / split / 'labels'
            
            print(f"\n{split.upper()} ì„¸íŠ¸:")
            print(f"  ì´ë¯¸ì§€ í´ë”: {images_path.exists()} - {images_path.absolute()}")
            print(f"  ë¼ë²¨ í´ë”: {labels_path.exists()} - {labels_path.absolute()}")
            
            if images_path.exists():
                image_files = list(images_path.glob('*'))
                print(f"  ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: {len(image_files)}")
                if image_files:
                    print(f"  ì²« ë²ˆì§¸ íŒŒì¼: {image_files[0].name}")
        
        # data.yaml í™•ì¸
        yaml_path = dataset_path / 'data.yaml'
        if yaml_path.exists():
            print(f"\ndata.yaml ì¡´ì¬: {yaml_path.absolute()}")
            with open(yaml_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            print("data.yaml ë‚´ìš©:")
            for key, value in data.items():
                print(f"  {key}: {value}")
        else:
            print("\ndata.yaml íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    print("=" * 50)

def manual_fix_yaml():
    """ìˆ˜ë™ìœ¼ë¡œ data.yaml íŒŒì¼ ìˆ˜ì •"""
    dataset_folder = "generated_animal_dataset"
    dataset_path = Path(dataset_folder)
    yaml_path = dataset_path / 'data.yaml'
    
    # í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œ
    current_dir = Path.cwd()
    
    # ìƒˆë¡œìš´ data.yaml ìƒì„±
    new_data = {
        'train': str(current_dir / dataset_folder / 'train' / 'images'),
        'val': str(current_dir / dataset_folder / 'val' / 'images'),
        'test': str(current_dir / dataset_folder / 'test' / 'images'),
        'nc': 2,  # í´ë˜ìŠ¤ ìˆ˜ (target1, target2)
        'names': ['target1', 'target2']  # í´ë˜ìŠ¤ ì´ë¦„
    }
    
    # ì‹¤ì œ í´ë˜ìŠ¤ ì •ë³´ê°€ ìˆë‹¤ë©´ ê¸°ì¡´ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¤ê¸°
    if yaml_path.exists():
        try:
            with open(yaml_path, 'r', encoding='utf-8') as f:
                existing_data = yaml.safe_load(f)
            if 'nc' in existing_data:
                new_data['nc'] = existing_data['nc']
            if 'names' in existing_data:
                new_data['names'] = existing_data['names']
        except:
            pass
    
    # ìƒˆ íŒŒì¼ ì €ì¥
    with open(yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(new_data, f, default_flow_style=False)
    
    print(f"data.yaml íŒŒì¼ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {yaml_path}")
    print("ìƒˆë¡œìš´ ê²½ë¡œ:")
    for key, value in new_data.items():
        print(f"  {key}: {value}")


class AnimalDatasetGenerator:
    def __init__(self, model_path, main_folder, output_folder="generated_dataset"):
        """
        ë™ë¬¼ ê°ì§€ ê¸°ë°˜ ìë™ ë°ì´í„°ì…‹ ìƒì„±ê¸°
        
        Args:
            model_path (str): í•™ìŠµëœ animal ëª¨ë¸ ê²½ë¡œ (ì˜ˆ: 'animal.pt')
            main_folder (str): target1, target2 ë“±ì˜ í´ë”ê°€ ìˆëŠ” ë©”ì¸ í´ë” ê²½ë¡œ
            output_folder (str): ìƒì„±ë  ë°ì´í„°ì…‹ í´ë” ì´ë¦„
        """
        self.model = YOLO(model_path)
        self.main_folder = Path(main_folder)
        self.output_folder = Path(output_folder)
        
        # ì§€ì›í•˜ëŠ” ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ í™•ì¥ì
        self.image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        self.video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv'}
        
        # ë°ì´í„°ì…‹ í´ë” êµ¬ì¡° ìƒì„±
        self.setup_dataset_structure()
        
        # í´ë˜ìŠ¤ ì •ë³´ ì €ì¥
        self.class_names = []
        self.class_count = 0
        
    def setup_dataset_structure(self):
        """YOLO í˜•ì‹ì˜ ë°ì´í„°ì…‹ í´ë” êµ¬ì¡° ìƒì„±"""
        # ë©”ì¸ ë°ì´í„°ì…‹ í´ë” ìƒì„±
        self.output_folder.mkdir(exist_ok=True)
        
        # train, val, test í´ë” ìƒì„±
        for split in ['train', 'val', 'test']:
            (self.output_folder / split / 'images').mkdir(parents=True, exist_ok=True)
            (self.output_folder / split / 'labels').mkdir(parents=True, exist_ok=True)
    
    def get_target_folders(self):
        """ë©”ì¸ í´ë”ì—ì„œ targetë¡œ ì‹œì‘í•˜ëŠ” í´ë”ë“¤ ì°¾ê¸°"""
        target_folders = []
        for folder in self.main_folder.iterdir():
            if folder.is_dir() and folder.name.startswith('target'):
                target_folders.append(folder)
        return target_folders
    
    def extract_frames_from_video(self, video_path, output_dir, frame_interval=30):
        """ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ"""
        cap = cv2.VideoCapture(str(video_path))
        frame_count = 0
        saved_frame_count = 0
        
        frames = []
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # ì§€ì •ëœ ê°„ê²©ë§ˆë‹¤ í”„ë ˆì„ ì €ì¥
            if frame_count % frame_interval == 0:
                frame_filename = f"{video_path.stem}_frame_{saved_frame_count:04d}.jpg"
                frame_path = output_dir / frame_filename
                cv2.imwrite(str(frame_path), frame)
                frames.append(frame_path)
                saved_frame_count += 1
            
            frame_count += 1
        
        cap.release()
        return frames
    
    def detect_animals_in_image(self, image_path, confidence_threshold=0.25):
        """ì´ë¯¸ì§€ì—ì„œ ë™ë¬¼ ê°ì§€"""
        results = self.model.predict(source=str(image_path), conf=confidence_threshold, verbose=False)
        
        if len(results) == 0 or len(results[0].boxes) == 0:
            return []
        
        detections = []
        for box in results[0].boxes:
            # YOLO í˜•ì‹ìœ¼ë¡œ ì¢Œí‘œ ë³€í™˜ (normalized coordinates)
            x_center = (box.xyxy[0][0] + box.xyxy[0][2]) / 2 / results[0].orig_shape[1]
            y_center = (box.xyxy[0][1] + box.xyxy[0][3]) / 2 / results[0].orig_shape[0]
            width = (box.xyxy[0][2] - box.xyxy[0][0]) / results[0].orig_shape[1]
            height = (box.xyxy[0][3] - box.xyxy[0][1]) / results[0].orig_shape[0]
            
            detections.append({
                'x_center': float(x_center),
                'y_center': float(y_center),
                'width': float(width),
                'height': float(height),
                'confidence': float(box.conf[0])
            })
        
        return detections
    
    def save_yolo_annotation(self, detections, class_id, annotation_path):
        """YOLO í˜•ì‹ì˜ ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ì €ì¥"""
        with open(annotation_path, 'w') as f:
            for detection in detections:
                line = f"{class_id} {detection['x_center']:.6f} {detection['y_center']:.6f} {detection['width']:.6f} {detection['height']:.6f}\n"
                f.write(line)
    
    def process_target_folder(self, target_folder, class_id, split='train'):
        """target í´ë” ë‚´ì˜ ëª¨ë“  ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ì²˜ë¦¬"""
        processed_count = 0
        
        # ì„ì‹œ í´ë” ìƒì„± (ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì¶œìš©)
        temp_frames_dir = self.output_folder / 'temp_frames'
        temp_frames_dir.mkdir(exist_ok=True)
        
        for file_path in target_folder.rglob('*'):
            if not file_path.is_file():
                continue
                
            file_extension = file_path.suffix.lower()
            
            # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬
            if file_extension in self.image_extensions:
                detections = self.detect_animals_in_image(file_path)
                
                if detections:  # ë™ë¬¼ì´ ê°ì§€ëœ ê²½ìš°ë§Œ ì²˜ë¦¬
                    # ì´ë¯¸ì§€ ë³µì‚¬
                    new_image_name = f"{target_folder.name}_{file_path.stem}_{processed_count:04d}.jpg"
                    new_image_path = self.output_folder / split / 'images' / new_image_name
                    shutil.copy2(file_path, new_image_path)
                    
                    # ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ìƒì„±
                    annotation_path = self.output_folder / split / 'labels' / f"{new_image_name.replace('.jpg', '.txt')}"
                    self.save_yolo_annotation(detections, class_id, annotation_path)
                    
                    processed_count += 1
                    print(f"ì²˜ë¦¬ë¨: {file_path.name} -> {new_image_name} (ë™ë¬¼ {len(detections)}ê°œ ê°ì§€)")
            
            # ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
            elif file_extension in self.video_extensions:
                print(f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘: {file_path.name}")
                
                # ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ
                frames = self.extract_frames_from_video(file_path, temp_frames_dir)
                
                for frame_path in frames:
                    detections = self.detect_animals_in_image(frame_path)
                    
                    if detections:  # ë™ë¬¼ì´ ê°ì§€ëœ ê²½ìš°ë§Œ ì²˜ë¦¬
                        # ì´ë¯¸ì§€ ë³µì‚¬
                        new_image_name = f"{target_folder.name}_{frame_path.stem}_{processed_count:04d}.jpg"
                        new_image_path = self.output_folder / split / 'images' / new_image_name
                        shutil.copy2(frame_path, new_image_path)
                        
                        # ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ìƒì„±
                        annotation_path = self.output_folder / split / 'labels' / f"{new_image_name.replace('.jpg', '.txt')}"
                        self.save_yolo_annotation(detections, class_id, annotation_path)
                        
                        processed_count += 1
                        print(f"ì²˜ë¦¬ë¨: {frame_path.name} -> {new_image_name} (ë™ë¬¼ {len(detections)}ê°œ ê°ì§€)")
                
                # ì„ì‹œ í”„ë ˆì„ íŒŒì¼ë“¤ ì‚­ì œ
                for frame_path in frames:
                    frame_path.unlink()
        
        # ì„ì‹œ í´ë” ì‚­ì œ
        if temp_frames_dir.exists():
            shutil.rmtree(temp_frames_dir)
        
        return processed_count
    
    def create_data_yaml(self):
        """YOLO í•™ìŠµìš© data.yaml íŒŒì¼ ìƒì„±"""
        data_yaml = {
            'train': str(self.output_folder / 'train' / 'images'),
            'val': str(self.output_folder / 'val' / 'images'),
            'test': str(self.output_folder / 'test' / 'images'),
            'nc': len(self.class_names),
            'names': self.class_names
        }
        
        yaml_path = self.output_folder / 'data.yaml'
        with open(yaml_path, 'w') as f:
            yaml.dump(data_yaml, f, default_flow_style=False)
        
        print(f"data.yaml íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {yaml_path}")
    
    def generate_dataset(self, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):
        """ì „ì²´ ë°ì´í„°ì…‹ ìƒì„± í”„ë¡œì„¸ìŠ¤"""
        print("ë°ì´í„°ì…‹ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # target í´ë”ë“¤ ì°¾ê¸°
        target_folders = self.get_target_folders()
        
        if not target_folders:
            print("targetìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ë°œê²¬ëœ target í´ë”: {[folder.name for folder in target_folders]}")
        
        # í´ë˜ìŠ¤ ì´ë¦„ ì„¤ì •
        self.class_names = [folder.name for folder in target_folders]
        self.class_count = len(self.class_names)
        
        total_processed = 0
        
        # ê° target í´ë” ì²˜ë¦¬
        for class_id, target_folder in enumerate(target_folders):
            print(f"\n{'='*50}")
            print(f"ì²˜ë¦¬ ì¤‘: {target_folder.name} (í´ë˜ìŠ¤ ID: {class_id})")
            print(f"{'='*50}")
            
            # ì„ì‹œë¡œ train splitì— ëª¨ë“  ë°ì´í„° ì €ì¥
            processed_count = self.process_target_folder(target_folder, class_id, 'train')
            total_processed += processed_count
            
            print(f"{target_folder.name} í´ë”ì—ì„œ {processed_count}ê°œ ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ")
        
        # ë°ì´í„° ë¶„í•  (train/val/test)
        self.split_dataset(train_ratio, val_ratio, test_ratio)
        
        # data.yaml íŒŒì¼ ìƒì„±
        self.create_data_yaml()
        
        print(f"\n{'='*50}")
        print("ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
        print(f"ì´ ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {total_processed}ê°œ")
        print(f"í´ë˜ìŠ¤ ìˆ˜: {self.class_count}ê°œ")
        print(f"í´ë˜ìŠ¤ ì´ë¦„: {self.class_names}")
        print(f"ë°ì´í„°ì…‹ ìœ„ì¹˜: {self.output_folder}")
        print(f"{'='*50}")
    
    def split_dataset(self, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):
        """ë°ì´í„°ì…‹ì„ train/val/testë¡œ ë¶„í• """
        print("\në°ì´í„°ì…‹ ë¶„í•  ì¤‘...")
        
        # train í´ë”ì˜ ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
        train_images = list((self.output_folder / 'train' / 'images').glob('*.jpg'))
        train_images.sort()
        
        total_images = len(train_images)
        
        # ë¶„í•  ì¸ë±ìŠ¤ ê³„ì‚°
        train_end = int(total_images * train_ratio)
        val_end = train_end + int(total_images * val_ratio)
        
        # val ë°ì´í„° ì´ë™
        for i in range(train_end, val_end):
            if i < len(train_images):
                image_path = train_images[i]
                label_path = self.output_folder / 'train' / 'labels' / f"{image_path.stem}.txt"
                
                # ì´ë¯¸ì§€ ì´ë™
                new_image_path = self.output_folder / 'val' / 'images' / image_path.name
                shutil.move(str(image_path), str(new_image_path))
                
                # ë¼ë²¨ ì´ë™
                if label_path.exists():
                    new_label_path = self.output_folder / 'val' / 'labels' / label_path.name
                    shutil.move(str(label_path), str(new_label_path))
        
        # test ë°ì´í„° ì´ë™
        for i in range(val_end, total_images):
            if i < len(train_images):
                image_path = train_images[i]
                label_path = self.output_folder / 'train' / 'labels' / f"{image_path.stem}.txt"
                
                # ì´ë¯¸ì§€ ì´ë™
                new_image_path = self.output_folder / 'test' / 'images' / image_path.name
                shutil.move(str(image_path), str(new_image_path))
                
                # ë¼ë²¨ ì´ë™
                if label_path.exists():
                    new_label_path = self.output_folder / 'test' / 'labels' / label_path.name
                    shutil.move(str(label_path), str(new_label_path))
        
        # ë¶„í•  ê²°ê³¼ ì¶œë ¥
        train_count = len(list((self.output_folder / 'train' / 'images').glob('*.jpg')))
        val_count = len(list((self.output_folder / 'val' / 'images').glob('*.jpg')))
        test_count = len(list((self.output_folder / 'test' / 'images').glob('*.jpg')))
        
        print(f"Train: {train_count}ê°œ, Val: {val_count}ê°œ, Test: {test_count}ê°œ")

def train_model(data_path):
    # ì„ì‹œ í…ŒìŠ¤íŠ¸ ì½”ë“œì…ë‹ˆë‹¤
    print(f"ğŸ“¦ Training model with data in {data_path}")
    # ì‚¬ìš© ì˜ˆì œ
    model_path = "animal.pt"  # í•™ìŠµëœ animal ëª¨ë¸ ê²½ë¡œ
    main_folder = "./train_data"  # target1, target2 í´ë”ê°€ ìˆëŠ” ë©”ì¸ í´ë”
    output_folder = "generated_animal_dataset"  # ìƒì„±ë  ë°ì´í„°ì…‹ í´ë”
    
    # ë°ì´í„°ì…‹ ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = AnimalDatasetGenerator(model_path, main_folder, output_folder)
    
    # ë°ì´í„°ì…‹ ìƒì„± (train:80%, val:10%, test:10%)
    generator.generate_dataset(train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)
    
    print("ë°ì´í„°ì…‹ ë¬¸ì œ í•´ê²° ë° í•™ìŠµ ì‹œì‘")
    
    # 1. ë°ì´í„°ì…‹ ê²½ë¡œ ë””ë²„ê¹…
    debug_dataset_paths()
    
    # 2. data.yaml ìˆ˜ë™ ìˆ˜ì •
    manual_fix_yaml()
    
    # 3. ìˆ˜ì •ëœ í•™ìŠµ ì‹¤í–‰
    fixed_train()

    return True, "Model trained successfully"

#################################################################

#ì „ì²´ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜
def run_training_pipeline(target_id):
    #1ë‹¨ê³„: ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
    success, result = download_images(target_id)
    if not success:
        return False, result

    data_path = result
    #2ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ ì½”ë“œ ë„£ì–´ì£¼ì„¸ìš” íŒŒì´íŒ…~!
    success, msg = train_model(data_path)
    if not success:
        return False, msg

    return True, msg